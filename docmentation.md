# Gemma Chatbot API ğŸ¤–

A conversational chatbot API powered by Google's Gemma-3-1B-IT model, built with FastAPI.

## ğŸ“‹ Project Status

### âœ… Completed (Person 1 - Backend Core)

**Files Created:**
- âœ… `backend/main.py` - FastAPI application setup
- âœ… `backend/config.py` - Configuration management
- âœ… `backend/api/routes.py` - All API endpoints
- âœ… `backend/models/schemas.py` - Request/Response models
- âœ… `backend/utils/helpers.py` - Utility functions
- âœ… `backend/__init__.py` - Package initialization
- âœ… `requirements.txt` - Python dependencies

**Features Implemented:**
- âœ… FastAPI application with CORS
- âœ… Health check endpoint (`GET /api/v1/health`)
- âœ… Chat endpoint (`POST /api/v1/chat`)
- âœ… Session management endpoints:
  - `GET /api/v1/sessions` - List all sessions
  - `GET /api/v1/sessions/{id}/info` - Get session info
  - `DELETE /api/v1/sessions/{id}` - Delete session
  - `POST /api/v1/sessions/{id}/clear-history` - Clear history
- âœ… Request/response validation with Pydantic
- âœ… Comprehensive error handling
- âœ… Logging infrastructure
- âœ… API documentation (Swagger/ReDoc)

### ğŸš§ TODO (Person 2 - Chat Service & Model)

**Files to Create:**
- â³ `backend/services/chat_service.py` - Main chat logic
- â³ `backend/services/__init__.py` - Services package

**Tasks:**
1. **Model Integration:**
   - Load `google/gemma-3-1b-it` model (HuggingFace or Ollama)
   - Set up tokenizer and generation pipeline
   - Configure model parameters (temperature, max_tokens, etc.)

2. **Chat Service Implementation:**
   - Implement `ChatService` class
   - Handle conversation history with LangChain
   - Create prompt templates
   - Implement `chat()` method for generating responses
   - Implement session management methods:
     - `get_session_history()`
     - `clear_history()`
     - `delete_session()`
     - `get_message_count()`
     - `session_exists()`
     - `is_model_loaded()`
     - `get_active_sessions_count()`
     - `get_all_sessions()`
     - `get_session_created_time()`

3. **Testing:**
   - Test model loading and inference
   - Test conversation flow
   - Optimize response quality

### ğŸ¨ TODO (Person 3 - Frontend & Testing)

**Files to Create:**
- â³ `frontend/index.html` - Chat interface
- â³ `frontend/app.js` - Frontend logic
- â³ `frontend/styles.css` - Styling
- â³ `tests/test_api.py` - API tests
- â³ `tests/test_chat.py` - Chat functionality tests

**Tasks:**
1. **Frontend:**
   - Create simple chat interface
   - Implement message sending/receiving
   - Display conversation history
   - Add session management UI

2. **Testing:**
   - Write unit tests for API endpoints
   - Write integration tests
   - Test edge cases and error handling

3. **Documentation:**
   - Add API usage examples
   - Create user guide
   - Document deployment process

---

## ğŸš€ Quick Start (Current State)

### Prerequisites
```bash
# Python 3.9+
python --version

# Virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Installation
```bash
# Install dependencies
pip install -r requirements.txt
```

### Configuration
Create a `.env` file in the project root:
```env
# API Settings
API_TITLE=Gemma Chatbot API
API_VERSION=1.0.0

# Model Settings
MODEL_NAME=google/gemma-3-1b-it
MODEL_DEVICE=cpu
TEMPERATURE=0.7
MAX_TOKENS=512

# Optional: HuggingFace Token
HUGGINGFACE_TOKEN=your_token_here
```

### Running the API (After Person 2 completes chat_service.py)
```bash
# Development mode
python -m backend.main

# Or using uvicorn directly
uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000
```

### Access
- **API Docs**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/api/v1/health

---

## ğŸ“ Project Structure

```
defi-project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py                 âœ… Done
â”‚   â”œâ”€â”€ main.py                     âœ… Done
â”‚   â”œâ”€â”€ config.py                   âœ… Done
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes.py               âœ… Done
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py              âœ… Done
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py             â³ Person 2
â”‚   â”‚   â””â”€â”€ chat_service.py         â³ Person 2
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py              âœ… Done
â”œâ”€â”€ frontend/                       â³ Person 3
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ tests/                          â³ Person 3
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_chat.py
â”œâ”€â”€ requirements.txt                âœ… Done
â”œâ”€â”€ .env                            â³ Create this
â””â”€â”€ README.md                       âœ… Done
```

---

## ğŸ§ª Testing Current Implementation

### Test 1: Health Check
```bash
curl http://localhost:8000/api/v1/health
```

**Expected Response (will show error until Person 2 completes chat_service):**
```json
{
  "status": "unhealthy",
  "error": "chat_service not initialized",
  "note": "Person 2 needs to implement chat_service.py"
}
```

### Test 2: Root Endpoint
```bash
curl http://localhost:8000/
```

**Expected Response:**
```json
{
  "message": "Welcome to Gemma Chatbot API",
  "version": "1.0.0",
  "model": "google/gemma-3-1b-it",
  "docs": "/docs",
  "health": "/api/v1/health"
}
```

### Test 3: API Documentation
Open browser: `http://localhost:8000/docs`

You should see:
- All endpoints documented
- Interactive API testing interface
- Request/response schemas

---

## ğŸ“ API Endpoints

### Chat
- **POST** `/api/v1/chat` - Send message and get response

### Sessions
- **GET** `/api/v1/sessions` - List all sessions
- **GET** `/api/v1/sessions/{id}/info` - Get session details
- **DELETE** `/api/v1/sessions/{id}` - Delete session
- **POST** `/api/v1/sessions/{id}/clear-history` - Clear chat history

### System
- **GET** `/api/v1/health` - Health check
- **GET** `/` - Root info

---

## ğŸ¤ Team Workflow

### Person 1 (You) - âœ… COMPLETED
Your work is done! The API infrastructure is ready.

### Person 2 - Next Steps
1. Clone the repo and pull Person 1's code
2. Create `backend/services/chat_service.py`
3. Implement model loading (HuggingFace or Ollama)
4. Implement ChatService class with all required methods
5. Test with API endpoints
6. Push code for Person 3

### Person 3 - After Person 2
1. Pull Person 2's code
2. Create simple frontend to test chatbot
3. Write comprehensive tests
4. Document everything
5. Prepare for deployment

---

## ğŸ”§ Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MODEL_NAME` | `google/gemma-3-1b-it` | Model identifier |
| `MODEL_DEVICE` | `cpu` | Device: `cpu` or `cuda` |
| `TEMPERATURE` | `0.7` | Response randomness (0-1) |
| `MAX_TOKENS` | `512` | Max response length |
| `MAX_HISTORY_LENGTH` | `10` | Conversation history limit |

---

## ğŸ“š Resources

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Gemma Model Card](https://huggingface.co/google/gemma-3-1b-it)
- [LangChain Docs](https://python.langchain.com/)
- [Pydantic Docs](https://docs.pydantic.dev/)

---

## ğŸ› Known Issues

- âš ï¸ Chat endpoint will return 500 error until `chat_service.py` is implemented
- âš ï¸ Health check will show "unhealthy" until model is loaded

---

## ğŸ“„ License

MIT License - feel free to use this project for learning and development.

---

**Status**: Person 1 work complete âœ… | Waiting for Person 2 to implement chat service ğŸš§